{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WesleySantosMaxtelll/doutorado/blob/main/lstm_setembroBR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNAcqOGL_nFl",
        "outputId": "ccd804c3-04a4-4574-b369-4fdb0fc5e554"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 32.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 65.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 77.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Nau59NnHCGM",
        "outputId": "2572d9ba-e595-4d4b-cc7d-a68b04c903f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EghDgLxX_aF1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW, SGD\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, BertConfig\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.utils import class_weight\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import time\n",
        "from datetime import datetime\n",
        "import gc\n",
        "from torch.nn import CrossEntropyLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6utzfwS1EuoL"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import random\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import OneClassSVM\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from numpy import asarray\n",
        "# nltk.download('stopwords')\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import re\n",
        "\n",
        "PATH_CORPUS = \"/content/drive/MyDrive/doutorado/dados_v6/\"\n",
        "\n",
        "\n",
        "def get_data_full():\n",
        "    f = pickle.load(open(PATH_CORPUS + '/depress-control_v3.pkl', 'rb'))\n",
        "    return f.Text, f.Diagnosed_YN.apply(lambda x: 1 if x == 'yes' else -1)\n",
        "\n",
        "\n",
        "def get_data_BERT(task):\n",
        "    f = pickle.load(open(PATH_CORPUS + f'/{task}-control_v3.pkl', 'rb'))\n",
        "    return f.User_ID, f.Text, f.Diagnosed_YN.apply(lambda x: 1 if x == 'yes' else -1)\n",
        "\n",
        "\n",
        "def get_data_BERT_one_round(task):\n",
        "    f_train = pickle.load(open(PATH_CORPUS + f'/train_{task}-control_v3.pkl', 'rb'))\n",
        "    text_train, label_train =  f_train.Text, f_train.Diagnosed_YN.apply(lambda x: 1 if x == 'yes' else 0)\n",
        "    f_test = pickle.load(open(PATH_CORPUS + f'/test_{task}-control_v3.pkl', 'rb'))\n",
        "    text_test, label_test = f_test.Text, f_test.Diagnosed_YN.apply(lambda x: 1 if x == 'yes' else 0)\n",
        "    return text_train, label_train, f_train.User_ID, text_test, label_test, f_test.User_ID\n",
        "\n",
        "\n",
        "def get_data_one_round(task, data_type=''):\n",
        "    d = {'depress': 'D', 'anxiety':\"A\"}\n",
        "    if data_type == 'nomention':\n",
        "        f_train = pickle.load(open(PATH_CORPUS + f'_no_mention/train_{task}-control_v3.pkl', 'rb'))\n",
        "        text_train, label_train = f_train.Text, f_train.Diagnosed_YN.apply(lambda x: 1 if x == 'yes' else -1)\n",
        "        f_test = pickle.load(open(PATH_CORPUS + f'/test_{task}-control_v3.pkl', 'rb'))\n",
        "        text_test, label_test = f_test.Text, f_test.Diagnosed_YN.apply(lambda x: 1 if x == 'yes' else -1)\n",
        "        return text_train, label_train, text_test, label_test\n",
        "    else:\n",
        "        f_train = pickle.load(open(PATH_CORPUS + f'/train_{d[task]}.pkl', 'rb'))\n",
        "        text_train, label_train =  f_train.Text, f_train.Diagnosed_YN.apply(lambda x: 1 if x == 'yes' else -1)\n",
        "        text_train = text_train.apply(lambda x: x.split('#'))\n",
        "        f_test = pickle.load(open(PATH_CORPUS + f'/test_{d[task]}.pkl', 'rb'))\n",
        "        text_test, label_test = f_test.Text, f_test.Diagnosed_YN.apply(lambda x: 1 if x == 'yes' else -1)\n",
        "        text_test = text_test.apply(lambda x: x.split('#'))\n",
        "        return text_train, label_train, text_test, label_test\n",
        "\n",
        "\n",
        "def get_data(task):\n",
        "    f = pickle.load(open(PATH_CORPUS + f'/train_{task}-control_v3.pkl', 'rb'))\n",
        "    return f.Text, f.Diagnosed_YN.apply(lambda x: 1 if x == 'yes' else -1)\n",
        "    # X_train, y_train, X_val, y_val, X_test, y_test = [], [], [], [], [], []\n",
        "    # for k in f['val_text'].keys():\n",
        "    #     X_val.append(f['val_text'][k])\n",
        "    #     y_val.append(f['val_label'][k])\n",
        "    # for k in f['train_text'].keys():\n",
        "    #     X_train.append(f['train_text'][k])\n",
        "    #     y_train.append(f['train_label'][k])\n",
        "    # for k in f['test_text'].keys():\n",
        "    #     X_test.append(f['test_text'][k])\n",
        "    #     y_test.append(f['test_label'][k])\n",
        "    # return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "\n",
        "# load embedding as a dict\n",
        "def load_embedding(filename):\n",
        "    # load embedding into memory, skip first line\n",
        "    file = open(filename, 'r')\n",
        "    lines = file.readlines()[1:]\n",
        "    file.close()\n",
        "    # create a map of words to vectors\n",
        "    embedding = dict()\n",
        "    for line in lines:\n",
        "        parts = line.split()\n",
        "        if len(parts) > 51:\n",
        "            continue\n",
        "        # key is string word, value is numpy array for vector\n",
        "        embedding[parts[0]] = asarray([i.replace(',','.') for i in parts[1:]], dtype='float32')\n",
        "    return embedding\n",
        "\n",
        "\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "\n",
        "def unison_shuffled_3_copies(a, b, c):\n",
        "    assert len(a) == len(b)\n",
        "    assert len(a) == len(c)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p], c[p]\n",
        "\n",
        "\n",
        "def get_representation(name):\n",
        "    if name == 'tfidf':\n",
        "        return TfidfVectorizer()\n",
        "    elif name == 'bow':\n",
        "        return CountVectorizer(min_df=10, stop_words=stopwords.words('portuguese'))\n",
        "\n",
        "\n",
        "def get_classifier(name):\n",
        "    if name == 'OCSVM':\n",
        "        return OneClassSVM(gamma='auto', kernel='poly')\n",
        "    if name == 'rl':\n",
        "        return LogisticRegression(class_weight='balanced')\n",
        "\n",
        "\n",
        "def split_by_tweet(df):\n",
        "    df_new = pd.concat([pd.Series(row['ids'], row['text'])\n",
        "                        for _, row in df.iterrows()]).reset_index()\n",
        "\n",
        "    df_new['label'] = df_new[0].apply(lambda x: 0 if 'C' in x else 1)\n",
        "    df_new = df_new.rename(columns={0: 'ids', 'label': 'label', 'index': 'text'})\n",
        "    return df_new\n",
        "\n",
        "\n",
        "# ao copiar/colar verifique se os caracteres de acentuaçao nas regexs estão corretos\n",
        "def makeTags(text):\n",
        "    text = str(text).lower()\n",
        "    if re.search(r\"diagn[oó]stic\", text, flags=re.DOTALL):\n",
        "        return 'diagnostico'\n",
        "    if re.search(r\"tratamento\", text, flags=re.DOTALL):\n",
        "        return 'tratamento'\n",
        "    if re.search(r\"depress[aã]o\", text, flags=re.DOTALL):\n",
        "        return 'depressao'\n",
        "    if re.search(r\"ansiedade\", text, flags=re.DOTALL):\n",
        "        return 'ansiedade'\n",
        "    if re.search(r\"depressivo|tarja\\sp|ansiol[íi]tico\", text, flags=re.DOTALL):\n",
        "        return 'antidepress'\n",
        "    if re.search(r\"m[eé]dic[oa]|psic[oó]l[oó]g[oa]|psiquiatra|neurologista|terapeuta\", text, flags=re.DOTALL):\n",
        "        return 'medico'\n",
        "    return ''\n",
        "\n",
        "\n",
        "def make_tag_feature(text, list_features):\n",
        "    text = str(text).lower()\n",
        "    combined = \"(\" + \")|(\".join(list_features) + \")\"\n",
        "    if re.match(combined, text, flags=re.DOTALL):\n",
        "        return 'found'\n",
        "    return ''\n",
        "\n",
        "\n",
        "# exemplo de uso - marca todos os textos que contêm palavras de natureza clínica\n",
        "def exemplo(df):\n",
        "    df['Event'] = ''\n",
        "    df['Event'] = df.apply(lambda x: [makeTags(t) for t in x['Text']], axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "def check_existance(df, list_features):\n",
        "    df['Event'] = ''\n",
        "    df['Event'] = df.apply(lambda x: [make_tag_feature(t, list_features) for t in x['Text']], axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, tokens_mask, labels):\n",
        "        self.tokens_mask = tokens_mask\n",
        "        self.labels = labels\n",
        "        self.size = 50\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]\n",
        "        data = self.tokens_mask[idx]\n",
        "        start = random.randint(0, data['tokens'].shape[0] - self.size)\n",
        "        data_short = {v: data[v][start: start + self.size] for v in data}\n",
        "        return [data_short, label]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQcQrn13D-QJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "\t\n",
        "    def __init__(self, args):\n",
        "        self.data = 'data/tweets.csv'\n",
        "        # self.data = 'data/tweets.csv'\n",
        "        self.max_len = args['max_len']\n",
        "        self.max_words = args['max_words']\n",
        "        self.test_size = args['test_size']\n",
        "        self.task = args['task']\n",
        "\n",
        "    def unison_shuffled_copies(self, a, b):\n",
        "        assert len(a) == len(b)\n",
        "        np.random.seed(42)\n",
        "        p = np.random.permutation(len(a))\n",
        "        return a[p], b[p]\n",
        "\n",
        "\n",
        "    def load_data(self, task):\n",
        "        text_train, label_train, text_test, label_test = get_data_one_round(task)\n",
        "        text_train, label_train, text_test, label_test = text_train.to_numpy(), label_train.to_numpy(), text_test.to_numpy(), label_test.to_numpy()\n",
        "        label_train[label_train == -1] = 0\n",
        "        label_test[label_test == -1] = 0\n",
        "        max_post = 200\n",
        "        np.random.seed(42)\n",
        "        # shuffle and select posts\n",
        "        text_train, label_train = unison_shuffled_copies(text_train, label_train)\n",
        "        for i in range(text_train.shape[0]):\n",
        "            text_train[i] = '\\n'.join(np.random.choice(text_train[i], min(len(text_train[i]), max_post), replace=False))\n",
        "\n",
        "        text_test, label_test = unison_shuffled_copies(text_test, label_test)\n",
        "        for i in range(text_test.shape[0]):\n",
        "            text_test[i] = '\\n'.join(np.random.choice(text_test[i], min(len(text_test[i]), max_post), replace=False))\n",
        "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(text_train, label_train, test_size=0.1, shuffle=True)\n",
        "\n",
        "        self.x_test, self.y_test = text_test, label_test\n",
        "        # self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(X, Y, test_size=self.test_size)\n",
        "        \n",
        "    def prepare_tokens(self):\n",
        "        self.tokens = Tokenizer(num_words=self.max_words)\n",
        "        self.tokens.fit_on_texts(self.x_train)\n",
        "\n",
        "    def sequence_to_token(self, x):\n",
        "        sequences = self.tokens.texts_to_sequences(x)\n",
        "        return sequence.pad_sequences(sequences, maxlen=self.max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0ZO9LlKCwOw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TweetClassifier(nn.ModuleList):\n",
        "    def __init__(self, args):\n",
        "        super(TweetClassifier, self).__init__()\n",
        "        \n",
        "        self.batch_size = args['batch_size']\n",
        "        self.hidden_dim = args['hidden_dim']\n",
        "        self.LSTM_layers = args['lstm_layers']\n",
        "        self.input_size = args['max_words'] # embedding dimention\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim,\n",
        "                  num_layers=self.LSTM_layers, batch_first=True)\n",
        "        self.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=100)\n",
        "        self.fc2 = nn.Linear(100 , 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "    \n",
        "        h = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n",
        "        c = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n",
        "        \n",
        "        torch.nn.init.xavier_normal_(h)\n",
        "        torch.nn.init.xavier_normal_(c)\n",
        "\n",
        "        out = self.embedding(x)\n",
        "        out, (hidden, cell) = self.lstm(out, (h,c))\n",
        "        out = self.dropout(out)\n",
        "        out = torch.relu(self.fc1(out[:,-1,:]))\n",
        "        out = self.dropout(out)\n",
        "        out = torch.softmax(self.fc2(out), 1)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8-Pw4AxCk05"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import WeightedRandomSampler, RandomSampler\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "class DatasetMaper(Dataset):\n",
        "    '''\n",
        "    Handles batches of dataset\n",
        "    '''\n",
        "\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "\n",
        "\n",
        "class Execute:\n",
        "    '''\n",
        "    Class for execution. Initializes the preprocessing as well as the\n",
        "    Tweet Classifier model\n",
        "    '''\n",
        "\n",
        "    def __init__(self, args):\n",
        "        print(f'Training  on task {args[\"task\"]}')\n",
        "        self.__init_data__(args)\n",
        "\n",
        "        self.args = args\n",
        "        self.batch_size = args['batch_size']\n",
        "\n",
        "        self.model = TweetClassifier(args)\n",
        "        # self.model.to(device)\n",
        "\n",
        "    def __init_data__(self, args):\n",
        "        '''\n",
        "        Initialize preprocessing from raw dataset to dataset split into training and testing\n",
        "        Training and test datasets are index strings that refer to tokens\n",
        "        '''\n",
        "        self.preprocessing = Preprocessing(args)\n",
        "        self.preprocessing.load_data(args['task'])\n",
        "        self.preprocessing.prepare_tokens()\n",
        "\n",
        "        raw_x_train = self.preprocessing.x_train\n",
        "        raw_x_test = self.preprocessing.x_test\n",
        "        raw_x_val = self.preprocessing.x_val\n",
        "\n",
        "        self.y_train = self.preprocessing.y_train\n",
        "        self.y_test = self.preprocessing.y_test\n",
        "        self.y_val = self.preprocessing.y_val\n",
        "\n",
        "        self.x_train = self.preprocessing.sequence_to_token(raw_x_train)\n",
        "        self.x_test = self.preprocessing.sequence_to_token(raw_x_test)\n",
        "        self.x_val = self.preprocessing.sequence_to_token(raw_x_val)\n",
        "\n",
        "    def train(self, test=False, path=''):\n",
        "\n",
        "        # Let there be 9 samples and 1 sample in class 0 and 1 respectively\n",
        "        class_counts = list(Counter(self.y_train).values())\n",
        "        num_samples = sum(class_counts)\n",
        "        labels = self.y_train  # corresponding labels of samples\n",
        "\n",
        "        class_weights = [num_samples / class_counts[i] for i in range(len(class_counts))]\n",
        "        weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n",
        "        sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
        "\n",
        "        training_set = DatasetMaper(self.x_train, self.y_train)\n",
        "        sampler = RandomSampler(training_set)\n",
        "        validation_set = DatasetMaper(self.x_val, self.y_val)\n",
        "        test_set = DatasetMaper(self.x_test, self.y_test)\n",
        "\n",
        "        self.loader_training = DataLoader(training_set, batch_size=self.batch_size, sampler=sampler)\n",
        "        self.loader_validation = DataLoader(validation_set)\n",
        "        self.loader_test = DataLoader(test_set)\n",
        "        if test:\n",
        "            output_structure = {'pred': None, 'true': None}\n",
        "\n",
        "            # self.model.load_state_dict(torch.load(path))\n",
        "            self.model = torch.load(f'{PATH_CORPUS}/lstm/{path}')\n",
        "            val_test = self.evaluation(self.loader_test)\n",
        "            pred = np.array(val_test).argmax(1)\n",
        "            output_structure['pred'] = pred\n",
        "            output_structure['true'] = self.y_test\n",
        "            print(f'Testing report on task {task}')\n",
        "            print(classification_report(self.y_test, pred))\n",
        "            pickle.dump(output_structure,\n",
        "                        open(f'{PATH_CORPUS}/lstm/predictions/{task}-LSTM.pkl', 'wb'))\n",
        "            return \n",
        "\n",
        "        # print(device)\n",
        "\n",
        "        optimizer = optim.RMSprop(self.model.parameters(), lr=args['learning_rate'])\n",
        "\n",
        "        def create_weight_dict(labels):\n",
        "            unique_labels = np.unique(labels)\n",
        "            class_weights = class_weight.compute_class_weight(\n",
        "                                  'balanced', classes=unique_labels, y=labels)\n",
        "            return class_weights\n",
        "        vect_weights = create_weight_dict(self.y_train)\n",
        "        loss_fct = CrossEntropyLoss(weight=torch.tensor(vect_weights, dtype=torch.float32))\n",
        "\n",
        "        print(f'{datetime.fromtimestamp(time.time())}\\t' + 'Start Training')\n",
        "        for epoch in range(args['epochs']):\n",
        "            predictions = []\n",
        "\n",
        "            self.model.train()\n",
        "            print('Total of ', len(self.loader_training))\n",
        "            for i, batch in enumerate(self.loader_training):\n",
        "                # batch = tuple(t.to(device) for t in batch)\n",
        "                if i % 10 ==0:\n",
        "                    print(f'{datetime.fromtimestamp(time.time())}\\t it {i}')\n",
        "                x_batch, y_batch = batch\n",
        "                x = x_batch.type(torch.LongTensor)\n",
        "                y = y_batch.type(torch.LongTensor).view(-1, 1)\n",
        "\n",
        "                y_pred = self.model(x)\n",
        "\n",
        "                loss = loss_fct(y_pred, y.view(-1))\n",
        "\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                predictions += list(y_pred.squeeze().detach().numpy())\n",
        "\n",
        "            val_predictions = self.evaluation(self.loader_validation)\n",
        "            pred = np.array(val_predictions).argmax(1)\n",
        "            # train_accuary = self.calculate_accuray(self.y_train, predictions)\n",
        "            # test_accuracy = self.calculate_accuray(self.y_test, val_predictions)\n",
        "            print(f'Validation report on task {task} and epoch {epoch}')\n",
        "            print(\n",
        "                f'{datetime.fromtimestamp(time.time())}\\t' + \"Epoch: %d, loss: %.5f\" % (epoch + 1, loss.item()))\n",
        "            w = [class_weights[i] for i in self.y_val]\n",
        "            print(classification_report(self.y_val, pred))\n",
        "            print(f'Weighted report')\n",
        "            print(classification_report(self.y_val, pred, sample_weight=w))\n",
        "            # print(f'Average Validation f1 {f1_score(self.y_val, pred)}')\n",
        "            torch.save(self.model, f'{PATH_CORPUS}/lstm/models/{task}-dim50-epoch:{epoch}.pt')\n",
        "\n",
        "    def evaluation(self, loader):\n",
        "        predictions = []\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in loader:\n",
        "                x = x_batch.type(torch.LongTensor)\n",
        "                y = y_batch.type(torch.FloatTensor)\n",
        "\n",
        "                y_pred = self.model(x)\n",
        "                predictions += list(y_pred.detach().numpy())\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_accuray(grand_truth, predictions):\n",
        "        true_positives = 0\n",
        "        true_negatives = 0\n",
        "\n",
        "        for true, pred in zip(grand_truth, predictions):\n",
        "            if (pred > 0.5) and (true == 1):\n",
        "                true_positives += 1\n",
        "            elif (pred < 0.5) and (true == 0):\n",
        "                true_negatives += 1\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "        return (true_positives + true_negatives) / len(grand_truth)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iNkLWO-FddR"
      },
      "outputs": [],
      "source": [
        "\n",
        "def parameter_parser():  \n",
        "    parser = {}\n",
        "    parser[\"epochs\"] = 15\n",
        "    parser[\"learning_rate\"] = 0.001\n",
        "    parser[\"hidden_dim\"] = 50\n",
        "    parser[\"lstm_layers\"] = 2\n",
        "    parser[\"batch_size\"] = 128\n",
        "    parser[\"test_size\"] = 0.2\n",
        "    parser[\"max_len\"] = 4000\n",
        "    parser[\"max_words\"] = 10_000\n",
        "    parser[\"task\"] = 'depress'\n",
        "    return parser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iePkeRVzCoiw",
        "outputId": "c85cf029-731b-422e-8e05-aeca268103f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start\n",
            "Got\n",
            "Training  on task anxiety\n",
            "Testing report on task anxiety\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.79      0.83      3108\n",
            "           1       0.13      0.23      0.17       444\n",
            "\n",
            "    accuracy                           0.72      3552\n",
            "   macro avg       0.50      0.51      0.50      3552\n",
            "weighted avg       0.78      0.72      0.75      3552\n",
            "\n",
            "Start\n",
            "Got\n",
            "Training  on task depress\n",
            "Testing report on task depress\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.82      0.85      2359\n",
            "           1       0.17      0.26      0.20       337\n",
            "\n",
            "    accuracy                           0.75      2696\n",
            "   macro avg       0.53      0.54      0.53      2696\n",
            "weighted avg       0.80      0.75      0.77      2696\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    problems = ['anxiety', 'depress']\n",
        "    dict_best_model = {'anxiety': 'models/anxiety-dim50-epoch:10.pt',\n",
        "                       'depress': 'models/depress-dim50-epoch:10.pt'}\n",
        "    for task in problems:\n",
        "        print('Start')\n",
        "        args = parameter_parser()\n",
        "        print('Got')\n",
        "        args['task'] = task\n",
        "        execute = Execute(args)\n",
        "        execute.train(True, dict_best_model[task])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "lstm_setembroBR.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNE3a+q3eTuiKo5G2DXHj2C",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}